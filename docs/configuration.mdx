---
title: "Configuration"
description: "Learn how to configure StruktX applications with factories and handlers"
---

<Info>
  StruktX uses a flexible factory-based configuration system. You can pass factories as callables, classes, instances, or import strings like <code>
  
  "module:attr"</code>

  . Plain dicts for component configs are also accepted and coerced into dataclasses.
</Info>

## StruktConfig

The main configuration object accepts factories for all components:

```python
StruktConfig(
    llm=LLMClientConfig(factory="my_pkg.llm:MyLLM"),
    classifier=ClassifierConfig(factory="my_pkg.classifiers:MyClassifier"),
    handlers=HandlersConfig(
        registry={"my_service": MyHandler},
        default_route="general",
    ),
    memory=MemoryConfig(factory="my_pkg.memory:MyMemory"),
    middleware=[MiddlewareConfig("my_pkg.middleware:MyMiddleware")],
)
```

## Factory System

Factories can be specified in two ways:

<CodeGroup>

```python lambda
# Using a class name
factory="my_pkg.module:MyComponent"

# Using a regular factory function
def create_component(**kwargs):
    return MyComponent(**kwargs)

factory=create_component
```


```python module
# Import from module
factory="my_pkg.module:MyFactory"

# Import with specific function
factory="my_pkg.llm:create_llm_client"

# Importing a class
factory="my_pkg.llm:MyLLMClass"
```

</CodeGroup>

<Note>
  Builder progressively injects `llm`, `memory`, and `store` and store into factories if their signatures accept them. Additional `params`are forwarded.
</Note>

## Handler Configuration

### Basic Handler Registry

```python
HandlersConfig(
    registry={
        "time_service": TimeHandler,
        "weather_service": WeatherHandler,
        "general": GeneralHandler,
    },
    default_route="general",
)
```

### Handler Parameters

Use `HandlersConfig.handler_params` for per-handler constructor arguments:

```python
HandlersConfig(
    registry={
        "my_service": MyHandler,
    },
    handler_params={
        "my_service": {
            "system": "You are a helpful assistant",
            "temperature": 0.7,
            "max_tokens": 1000
        }
    }
)
```

## LLM Configuration

### Basic LLM Setup

```python
LLMClientConfig(
    factory="langchain_openai:ChatOpenAI",
    params={"model": "gpt-4o-mini"}
)
```

### LangChain Integration

```python
from langchain_openai import ChatOpenAI
# You can pass LangChain models directly; StruktX adapts them automatically
LLMClientConfig(
    factory="langchain_openai:ChatOpenAI",
    params={"model": "gpt-4o-mini"}
)
```

## Classifier Configuration

```python
from strukt.classifiers.llm_classifier import (
DefaultLLMClassifier,
DEFAULT_CLASSIFIER_TEMPLATE)

ClassifierConfig(
    factory="strukt.classifiers.llm_classifier:DefaultLLMClassifier",
    params={
        "prompt_template": DEFAULT_CLASSIFIER_TEMPLATE,
        "allowed_types": ["time_service", "weather", "general"],
        "max_parts": 4,
    }
)
```

## Memory Configuration

### Vector Store Memory

```python
from strukt.memory import UpstashVectorMemoryEngine

MemoryConfig(
  factory="strukt.memory:UpstashVectorMemoryEngine",
  params={
    "index_url": "<UPSTASH_VECTOR_INDEX_URL>",
    "index_token": "<UPSTASH_VECTOR_TOKEN>",
    "namespace": "app1"
  },
  use_store=True,       # create a bound KnowledgeStore
  augment_llm=True,     # automatically inject scoped memory into prompts
)
```

<Info>
  With `augment_llm=True`, all LLM calls receive relevant memory snippets automatically. Provide `context`with `user_id`and optional `unit_id`for scoping.
</Info>

## Middleware Configuration

### Single Middleware

```python
middleware=[
    MiddlewareConfig("strukt.examples.middleware:LoggingMiddleware", params={"verbose": True})
]
```

### MiddlewareConfig

```python
from strukt.middleware import MiddlewareConfig

# Factory can be callable, class, instance, or import string "module:attr"; params are optional
MiddlewareConfig(
  factory="strukt.examples.middleware:MemoryExtractionMiddleware",
  params={"max_items": 5}
)
```

### Multiple Middleware

```python
middleware=[
    MiddlewareConfig("strukt.examples.middleware:AuthMiddleware", params={"valid_tokens": {"token1", "token2"}}),
    MiddlewareConfig("strukt.examples.middleware:RateLimitMiddleware", params={"max_requests": 10, "window_seconds": 60}),
    MiddlewareConfig("strukt.examples.middleware:LoggingMiddleware", params={"log_file": "app.log"}),
]
```

## Complete Example

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, ClassifierConfig, MemoryConfig, MiddlewareConfig
from strukt.classifiers.llm_classifier import DefaultLLMClassifier, DEFAULT_CLASSIFIER_TEMPLATE
from strukt.examples.time_handler import TimeHandler
from strukt.examples.middleware import MemoryExtractionMiddleware

import os
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
```

```python
# LLM
llm = LLMClientConfig("langchain_openai:ChatOpenAI", dict(model="gpt-4o-mini"))
```

```python
# Classifier
classifier = ClassifierConfig(
  DefaultLLMClassifier,
  dict(
    prompt_template=DEFAULT_CLASSIFIER_TEMPLATE,
    allowed_types=["time_service", "weather", "general", "memory_extraction"],
    max_parts=4,
  ),
)
```

```python
# Memory (Upstash Vector)
memory = MemoryConfig(
  factory="strukt.memory:UpstashVectorMemoryEngine",
  params=dict(index_url="...", index_token="..."),
  use_store=True,
  augment_llm=True,
)
```

```python
# Handlers
handlers = HandlersConfig({"time_service": TimeHandler}, default_route="general")
```

```python
# Middleware
middleware = [MiddlewareConfig("strukt.examples.middleware:MemoryExtractionMiddleware")]
```

```python
# Assemble
app = create(StruktConfig(
  llm=llm,
  classifier=classifier,
  memory=memory,
  handlers=handlers,
  middleware=middleware,
))

# Invoke (memory will be auto-injected into LLM if augment_llm=True)
app.invoke("I currently live in Beirut, what is the time there?")
```

<Info>
  All configuration components are optional except for the classifier and handlers. You can start with minimal configuration and add components as needed.
</Info>

## Environment Variables

For sensitive configuration like API keys, use environment variables:

```python
import os
from langchain_openai import ChatOpenAI

# Load from environment
api_key = os.getenv("OPENAI_API_KEY")
langchain_llm = ChatOpenAI(api_key=api_key)

LLMClientConfig(
    factory="langchain_openai:ChatOpenAI",
    params={"api_key": api_key, "model": "gpt-4o-mini"}
)
```

## Configuration Best Practices

<Steps>
  <Step title="Start Simple">
    Begin with basic configuration and add complexity as needed
  </Step>
  <Step title="Use Factories">
    Leverage the factory system for flexible component creation
  </Step>
  <Step title="Environment Variables">
    Store sensitive data in environment variables
  </Step>
  <Step title="Type Safety">
    Use type hints and Pydantic models for better development experience
  </Step>
  <Step title="Modular Design">
    Organize handlers and components by functionality
  </Step>
</Steps>