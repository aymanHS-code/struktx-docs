---
title: "Introduction"
description: "A configurable, typed AI framework with swappable LLM, classifier, handlers, and optional memory"
---

# StruktX

A configurable, typed AI framework with swappable LLM, classifier, handlers, and optional memory.

<CardGroup cols={2}>
  <Card title="Swappable Components" icon="arrows-repeat" iconType="regular" href="/configuration">
    LLM clients, classifiers, handlers, and memory engines
  </Card>
  <Card title="Type Safety" icon="note-sticky" href="/configuration">
    Fully typed system alongside Pydantic-first models
  </Card>
  <Card title="Configurable" icon="wrench-simple" href="/configuration">
    Factory-based configuration system (callable, class, instance, or import string)
  </Card>
  <Card title="Extensible" icon="plug" href="/examples">
    Easy to add custom components
  </Card>
  <Card title="Scoped Memory" icon="head-side-brain" href="/configuration#memory-configuration">
    Upstash Vector, KnowledgeStore, and context-aware injection
  </Card>
  <Card title="Middleware" icon="arrow-progress" href="/middleware">
    Pre/post processing hooks (including Memory Extraction)
  </Card>
</CardGroup>

## Quick Install

<Tabs>
  <Tab title="uv (Recommended)">
    ```bash
    # Install with core dependencies only
    uv pip install struktx
    
    # Install with LLM support (LangChain)
    uv pip install struktx[llm]
    
    # Install with vector store support
    uv pip install struktx[vector]
    
    # Install with all optional dependencies
    uv pip install struktx[all]
    
    # Install for development
    uv pip install struktx[dev]
    ```
  </Tab>
  <Tab title="pip">
    ```bash
    # Install with core dependencies only
    pip install struktx
    
    # Install with LLM support (LangChain)
    pip install struktx[llm]
    
    # Install with vector store support
    uv pip install struktx[vector]
    
    # Install with all optional dependencies
    pip install struktx[all]
    ```
  </Tab>
</Tabs>

## Quick Start

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, ClassifierConfig
from strukt.classifiers.llm_classifier import DefaultLLMClassifier, DEFAULT_CLASSIFIER_TEMPLATE
from strukt.examples.time_handler import TimeHandler

import os
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
```

```python
# LLM
llm = LLMClientConfig(
  "langchain_openai:ChatOpenAI",
  dict(model="gpt-4o-mini"),
)
```

```python
# Classifier
classifier = ClassifierConfig(
  DefaultLLMClassifier,
  dict(
    prompt_template=DEFAULT_CLASSIFIER_TEMPLATE,
    allowed_types=["time_service", "general"],
    max_parts=4,
  ),
)
```

```python
# Handlers
handlers = HandlersConfig(
  {"time_service": TimeHandler},
  default_route="general",
)
```

```python
# Create app
app = create(StruktConfig(
  llm=llm,
  classifier=classifier,
  handlers=handlers,
))

app.invoke("what is the time in Beirut?", context={"user_id": "123"})
```

<Info>
  If <code>
  OPENAI_API_KEY</code>

   is set and no <code>
  llm</code>

   is provided, a sensible default LLM client is selected automatically. Otherwise, a minimal <code>
  SimpleLLMClient</code>

   is used.
</Info>

## Quick Start with Memory \+ Middleware

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, ClassifierConfig, MemoryConfig
from strukt.classifiers.llm_classifier import DefaultLLMClassifier
from strukt.examples.time_handler import TimeHandler
from strukt.middleware import MiddlewareConfig

app = create(StruktConfig(
  llm=LLMClientConfig("langchain_openai:ChatOpenAI", dict(model="gpt-4o-mini")),
  classifier=ClassifierConfig(DefaultLLMClassifier,
                             dict(max_parts=4,
                             allowed_types=["time_service", "general", "memory_extraction"])),
  memory=MemoryConfig(
    factory="strukt.memory:UpstashVectorMemoryEngine",
    params=dict(index_url="https://...", index_token="...", namespace="app1"),
    use_store=True,
    augment_llm=True,
  ),
  handlers=HandlersConfig(dict(time_service=TimeHandler), default_route="general"),
  middleware=[MiddlewareConfig("strukt.examples.middleware:MemoryExtractionMiddleware")],
))

result = app.invoke("I live in Beirut, what's the time?", context={"user_id": "u1", "unit_id": "apt-101"})
```

<Info>
  Memory can be scoped by <code>
  user_id</code>

   and <code>
  unit_id</code>

  . With <code>
  augment_llm=True</code>

  , StruktX automatically injects relevant, scoped memory into prompts.
</Info>

## What is StruktX?

StruktX is a Python framework for building AI applications with a clean, modular architecture. It provides:

- **LLM Integration**: Support for any LLM via the `LLMClient` interface
- **Query Classification**: Route requests to appropriate handlers
- **Structured Outputs**: Pydantic model integration
- **Memory Engines**: Conversation history and context
- **Middleware System**: Pre/post processing hooks
- **LangChain Helpers**: Easy integration with LangChain ecosystem

## Requirements

- Python 3.8.1\+
- Core: `pydantic>=2.0.0`, `python-dotenv>=1.0.0`
- Optional: LangChain packages, vector stores, etc.

<Note>
  StruktX is designed to be lightweight and modular. Install only the dependencies you need for your specific use case.
</Note>