---
title: "Examples"
description: "Practical examples of StruktX configurations and usage patterns"
---

## Basic Setup with LangChain

A complete example using LangChain with OpenAI:

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, ClassifierConfig
from strukt.classifiers.llm_classifier import DefaultLLMClassifier, DEFAULT_CLASSIFIER_TEMPLATE
from strukt.examples.time_handler import TimeHandler

import os
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
```

```python
# LLM
llm = LLMClientConfig(
  "langchain_openai:ChatOpenAI",
  dict(model="gpt-4o-mini"),
)
```

```python
# Classifier
classifier = ClassifierConfig(
  DefaultLLMClassifier,
  dict(
    prompt_template=DEFAULT_CLASSIFIER_TEMPLATE,
    allowed_types=["time_service", "general"],
    max_parts=4,
  ),
)
```

```python
# Handlers
handlers = HandlersConfig(
  {"time_service": TimeHandler},
  default_route="general",
)
```

```python
# Create app
app = create(StruktConfig(
  llm=llm,
  classifier=classifier,
  handlers=handlers,
))

app.invoke("what is the time in Beirut?")
```

## Upstash Vector Memory + Scoped Injection

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, ClassifierConfig, MemoryConfig
from strukt.classifiers.llm_classifier import DefaultLLMClassifier
from strukt.examples.time_handler import TimeHandler
from strukt.middleware import MiddlewareConfig

# LLM
llm = LLMClientConfig("langchain_openai:ChatOpenAI", dict(model="gpt-4o-mini"))

# Classifier
classifier = ClassifierConfig(DefaultLLMClassifier, dict(max_parts=4, allowed_types=["time_service", "general", "memory_extraction"]))

# Memory
memory = MemoryConfig(
  factory="strukt.memory:UpstashVectorMemoryEngine",
  params={"index_url": "https://...", "index_token": "...", "namespace": "app1"},
  use_store=True,
  augment_llm=True,
)

# Handlers
handlers = HandlersConfig({"time_service": TimeHandler}, default_route="general")

# Middleware
middleware = [MiddlewareConfig("strukt.examples.middleware:MemoryExtractionMiddleware")]

app = create(StruktConfig(llm=llm, classifier=classifier, memory=memory, handlers=handlers, middleware=middleware))

# Memory-aware result
result = app.invoke("I live in Beirut, what's the time?", context={"user_id": "u1", "unit_id": "apt-101"})
```

<Info>
  Memory is automatically injected into LLM prompts using <code>user_id</code> and <code>unit_id</code> from <code>context</code>. Use <code>app.add_memory</code> or the memory extraction middleware to populate durable memories.
</Info>

## Custom Handler Example

Creating a custom handler for weather information:

```python
from pydantic import BaseModel
from strukt.interfaces import Handler, LLMClient
from strukt.types import InvocationState, HandlerResult
from strukt.langchain_helpers import create_structured_chain

class WeatherResponse(BaseModel):
    temperature: float
    condition: str
    location: str
    description: str

class WeatherHandler(Handler):
    def __init__(self, llm: LLMClient, api_key: str):
        self.llm = llm
        self.api_key = api_key

    def handle(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        location = parts[0] if parts else "New York"
        
        # Create structured chain for weather response
        chain = create_structured_chain(
            llm_client=self.llm,
            prompt_template=f"""
            You are a weather assistant. Provide weather information for {location}.
            Return the information in a structured format.
            """,
            output_model=WeatherResponse,
            input_variables=["location"],
        )
        
        # Get weather data (in real app, you'd call a weather API)
        weather_data = self._get_weather_data(location)
        
        # Generate response using LLM
        response = chain.invoke({"location": location})
        
        return HandlerResult(
            response=f"Weather in {response.location}: {response.temperature}Â°F, {response.condition}. {response.description}",
            status="weather_service"
        )
    
    def _get_weather_data(self, location: str) -> dict:
        # Mock weather data - replace with actual API call
        return {"temperature": 72, "condition": "Sunny"}
```

## Custom Classifier Example

Creating a custom classifier for specific use cases:

```python
from strukt.interfaces import Classifier
from strukt.types import InvocationState, QueryClassification
import re

class IntentClassifier(Classifier):
    def __init__(self, intents: dict[str, list[str]]):
        self.intents = intents
    
    def classify(self, state: InvocationState) -> QueryClassification:
        text = state.text.lower()
        
        # Check for exact matches
        for intent, keywords in self.intents.items():
            if any(keyword in text for keyword in keywords):
                return QueryClassification(
                    query_type=intent,
                    parts=[intent, text],
                    confidence=0.9
                )
        
        # Check for patterns
        if re.search(r'\b(time|hour|clock)\b', text):
            return QueryClassification(
                query_type="time_service",
                parts=["time", text],
                confidence=0.8
            )
        
        if re.search(r'\b(weather|temperature|forecast)\b', text):
            return QueryClassification(
                query_type="weather_service",
                parts=["weather", text],
                confidence=0.8
            )
        
        # Default to general
        return QueryClassification(
            query_type="general",
            parts=[text],
            confidence=0.5
        )

# Usage
intents = {
    "time_service": ["time", "hour", "clock", "schedule"],
    "weather_service": ["weather", "temperature", "forecast", "rain"],
    "help_service": ["help", "support", "assist", "guide"]
}

classifier = IntentClassifier(intents)
```

## Memory Integration Example

Adding conversation memory to your application:

```python
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig, MemoryConfig

class ConversationalHandler(Handler):
    def __init__(self, llm: LLMClient, memory: MemoryEngine):
        self.llm = llm
        self.memory = memory
    
    def handle(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        user_id = state.context.get("user_id", "default")
        
        # Get conversation history
        history = self.memory.get(user_id)
        
        # Build context from history
        context = ""
        if history:
            context = "\n".join([
                f"User: {msg['user']}\nAssistant: {msg['assistant']}"
                for msg in history[-5:]  # Last 5 messages
            ])
        
        # Generate response with context
        prompt = f"""
        Previous conversation:
        {context}
        
        Current user message: {state.text}
        
        Respond naturally, considering the conversation history.
        """
        
        response = self.llm.invoke(prompt)
        
        # Store in memory
        self.memory.add(user_id, {
            "user": state.text,
            "assistant": response,
            "timestamp": "2024-01-01T12:00:00Z"
        })
        
        return HandlerResult(response=response, status="conversational")

# Configuration with memory
app = create(StruktConfig(
  llm=LLMClientConfig(factory="langchain_openai:ChatOpenAI", params={"model": "gpt-4o-mini"}),
  classifier=dict(factory="strukt.classifiers.llm_classifier:DefaultLLMClassifier",
  params={"max_parts": 4, "allowed_types": ["time_service", "general", "memory_extraction"],
  "prompt_template": DEFAULT_CLASSIFIER_TEMPLATE}),
  handlers=HandlersConfig(
    registry={
      "conversational": ConversationalHandler,
    },
    default_route="conversational",
  ),
  memory=MemoryConfig(factory="strukt.memory.UpstashVectorMemoryEngine", augment_llm=False),
))
```

## Error Handling Example

Implementing robust error handling in handlers:

```python
from strukt.interfaces import Handler, LLMClient
from strukt.types import InvocationState, HandlerResult
import logging

logger = logging.getLogger(__name__)

class RobustHandler(Handler):
    def __init__(self, llm: LLMClient, fallback_response: str = "I'm sorry, I couldn't process that request."):
        self.llm = llm
        self.fallback_response = fallback_response
    
    def handle(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        try:
            # Validate input
            if not state.text.strip():
                return HandlerResult(
                    response="Please provide a valid input.",
                    status="error"
                )
            
            # Process request
            response = self.llm.invoke(state.text)
            
            # Validate response
            if not response or len(response.strip()) < 10:
                logger.warning(f"LLM returned short response: {response}")
                return HandlerResult(
                    response=self.fallback_response,
                    status="fallback"
                )
            
            return HandlerResult(response=response, status="success")
            
        except Exception as e:
            logger.error(f"Error processing request: {e}", exc_info=True)
            return HandlerResult(
                response=self.fallback_response,
                status="error"
            )
```

## Async Handler Example

Creating an async handler for I/O operations:

```python
import asyncio
import aiohttp
from strukt.interfaces import Handler, LLMClient
from strukt.types import InvocationState, HandlerResult

class AsyncWeatherHandler(Handler):
    def __init__(self, llm: LLMClient, api_key: str):
        self.llm = llm
        self.api_key = api_key
    
    async def handle_async(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        location = parts[0] if parts else "New York"
        
        # Async API call
        async with aiohttp.ClientSession() as session:
            url = f"https://api.weatherapi.com/v1/current.json?key={self.api_key}&q={location}"
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    weather_info = data['current']
                    
                    response_text = f"Weather in {location}: {weather_info['temp_f']}Â°F, {weather_info['condition']['text']}"
                    return HandlerResult(response=response_text, status="success")
                else:
                    return HandlerResult(
                        response=f"Sorry, I couldn't get weather data for {location}.",
                        status="error"
                    )
    
    def handle(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        # Run async handler in sync context (simplified for example)
        return asyncio.run(self.handle_async(state, parts))
```

## Testing Example

Testing your StruktX application:

```python
import pytest
from unittest.mock import Mock
from strukt import create, StruktConfig, HandlersConfig, LLMClientConfig

class TestHandler(Handler):
    def __init__(self, llm: LLMClient):
        self.llm = llm
    
    def handle(self, state: InvocationState, parts: list[str]) -> HandlerResult:
        return HandlerResult(
            response=f"Test response to: {state.text}",
            status="test"
        )

@pytest.fixture
def test_app():
    return create(StruktConfig(
        llm=LLMClientConfig(factory="langchain_openai:ChatOpenAI", params={"model": "gpt-4o-mini"}),
        classifier=dict(factory="strukt.classifiers.llm_classifier:DefaultLLMClassifier",
        params={"max_parts": 4, "allowed_types": ["time_service", "general", "memory_extraction"],
        "prompt_template": DEFAULT_CLASSIFIER_TEMPLATE}),
        handlers=HandlersConfig(
            registry={"test": TestHandler},
            default_route="test",
        )
    ))

def test_basic_invocation(test_app):
    result = test_app.invoke("Hello, world!", context={"user_id": "test_user"})
    assert result.response is not None
    assert "Hello, world!" in result.response

def test_context_preservation(test_app):
    context = {"user_id": "test_user", "session_id": "123"}
    result = test_app.invoke("Test message", context=context)
    assert result.context == context
```

<Info>
  These examples demonstrate various patterns and best practices. Adapt them to your specific use case and requirements.
</Info>